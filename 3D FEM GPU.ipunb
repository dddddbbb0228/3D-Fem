import numpy as np
import cupy as cp
import time
import torch
from torch.utils.cpp_extension import load_inline
from cupyx.scipy.sparse import csr_matrix, coo_matrix
from cupyx.scipy.sparse.linalg import cg
from GQ import TriGaussPoints3D  # 외부 가우스 적분 라이브러리

# ----------------------------------------------------------------
# 1. CUDA 커널 및 C++ 래퍼 소스 (통합)
# ----------------------------------------------------------------
# cpp_sources에는 선언부만, cuda_sources에는 구현부 전체를 넣습니다.
cpp_source = "void launch_assemble(torch::Tensor nodes, torch::Tensor elements, torch::Tensor A_ele, torch::Tensor det_out);"

cuda_source = r'''
__global__ void assemble_A_ele_kernel(
    const double* nodes, const int* elements,
    double* A_ele_out, double* det_out,
    int num_elements) {

    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i >= num_elements) return;

    // 1. 요소(Element)의 4개 노드 좌표 로드
    double c[4][3];
    for(int j=0; j<4; j++) {
        int idx = elements[i * 4 + j];
        c[j][0] = nodes[idx * 3 + 0];
        c[j][1] = nodes[idx * 3 + 1];
        c[j][2] = nodes[idx * 3 + 2];
    }

    // 2. Jacobian 및 Determinant 계산
    double j11 = c[1][0]-c[0][0]; double j12 = c[2][0]-c[0][0]; double j13 = c[3][0]-c[0][0];
    double j21 = c[1][1]-c[0][1]; double j22 = c[2][1]-c[0][1]; double j23 = c[3][1]-c[0][1];
    double j31 = c[1][2]-c[0][2]; double j32 = c[2][2]-c[0][2]; double j33 = c[3][2]-c[0][2];

    double detB = j11*(j22*j33 - j23*j32) - j12*(j21*j33 - j23*j31) + j13*(j21*j32 - j22*j31);
    double abs_det = (detB < 0) ? -detB : detB;
    det_out[i] = abs_det;

    // 3. 역행렬(B_inv) 계산을 통한 Shape Function Gradient 도출
    double invDet = 1.0 / detB;
    double i0 = (j22*j33-j23*j32)*invDet; double i1 = (j13*j32-j12*j33)*invDet; double i2 = (j12*j23-j13*j22)*invDet;
    double i3 = (j23*j31-j21*j33)*invDet; double i4 = (j11*j33-j13*j31)*invDet; double i5 = (j13*j21-j11*j23)*invDet;
    double i6 = (j21*j32-j22*j31)*invDet; double i7 = (j12*j31-j11*j32)*invDet; double i8 = (j11*j22-j12*j21)*invDet;

    double g[4][3];
    g[0][0]=-i0-i3-i6; g[0][1]=-i1-i4-i7; g[0][2]=-i2-i5-i8;
    g[1][0]=i0;        g[1][1]=i1;        g[1][2]=i2;
    g[2][0]=i3;        g[2][1]=i4;        g[2][2]=i5;
    g[3][0]=i6;        g[3][1]=i7;        g[3][2]=i8;

    // 4. 로컬 강성 행렬(A_ele) 계산 및 저장
    double vol_factor = abs_det / 6.0;
    for (int r=0; r<4; r++) {
        for (int k=0; k<4; k++) {
            double dot = g[r][0]*g[k][0] + g[r][1]*g[k][1] + g[r][2]*g[k][2];
            A_ele_out[i * 16 + r * 4 + k] = dot * vol_factor;
        }
    }
}

// PyTorch에서 호출할 래퍼 함수
void launch_assemble(torch::Tensor nodes, torch::Tensor elements, torch::Tensor A_ele, torch::Tensor det_out) {
    int num_elements = elements.size(0);
    const int threads = 256;
    const int blocks = (num_elements + threads - 1) / threads;

    assemble_A_ele_kernel<<<blocks, threads>>>(
        nodes.data_ptr<double>(), 
        elements.data_ptr<int>(),
        A_ele.data_ptr<double>(), 
        det_out.data_ptr<double>(), 
        num_elements
    );
}
'''

# ----------------------------------------------------------------
# 2. 데이터 로드 및 전처리
# ----------------------------------------------------------------
start_load = time.time()
step = 8  # 필요에 따라 64, 128로 변경

ele = np.loadtxt(f'./cube_Tri_h_{step}.txt', dtype=int) - 1
node = np.loadtxt(f'./cube_Node_h_{step}.txt', dtype=np.float64)

# 가우스 적분 점 데이터 처리 (배열 변환으로 튜플 에러 방지)
gauss_points_data = np.array(TriGaussPoints3D(2))
w = gauss_points_data[:, 3]
xi, yeta, zeta = gauss_points_data[:, 0], gauss_points_data[:, 1], gauss_points_data[:, 2]

print(f"LOAD 시간: {time.time() - start_load:.4f} 초")

# ----------------------------------------------------------------
# 3. GPU 가속 컴파일 및 Assembly
# ----------------------------------------------------------------
start_total = time.time()
start_pre = time.time()

# PyTorch 텐서로 변환 및 GPU 전송
node_gpu = torch.from_numpy(node).cuda()
ele_gpu = torch.from_numpy(ele.astype(np.int32)).cuda()  # int 타입 일치 중요
num_elements = ele.shape[0]

A_ele_gpu = torch.zeros((num_elements, 16), device='cuda', dtype=torch.float64)
det_gpu = torch.zeros(num_elements, device='cuda', dtype=torch.float64)

# 인라인 컴파일 수행
fem_mod = load_inline(
    name=f"fem_cuda_module_step_{step}",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=['launch_assemble'],
    with_cuda=True,
    verbose=False  # 상세 로그를 보려면 True로 변경
)

# CUDA 커널 실행
fem_mod.launch_assemble(node_gpu, ele_gpu, A_ele_gpu, det_gpu)

# --- 우변 b 계산 (CuPy 활용 벡터화) ---
def f_func(x, y, z):
    return cp.sin(cp.pi * x) * cp.sin(cp.pi * y) * cp.sin(cp.pi * z) * 3 * (cp.pi ** 2)

node_cp = cp.asarray(node)
ele_cp = cp.asarray(ele)
X, Y, Z = node_cp[ele_cp, 0], node_cp[ele_cp, 1], node_cp[ele_cp, 2]
phi_func = cp.array([1-xi-yeta-zeta, xi, yeta, zeta])

x_gp, y_gp, z_gp = X @ phi_func, Y @ phi_func, Z @ phi_func
f = f_func(x_gp, y_gp, z_gp)

# det_gpu를 CuPy로 변환하여 가중치 계산
weights = cp.asarray(det_gpu)[:, cp.newaxis] * cp.asarray(w)[cp.newaxis, :]
val = (f * weights) @ phi_func.T

b = cp.zeros(len(node))
b.scatter_add(ele_cp.flatten(), val.flatten())

end_pre = time.time()
elapsed_pre = end_pre - start_pre

# --- Sparse Matrix A 조립 (COO -> CSR) ---
start_assem = time.time()

I = ele_cp[:, :, cp.newaxis].repeat(4, axis=2).flatten()
J = ele_cp[:, cp.newaxis, :].repeat(4, axis=1).flatten()
V = cp.asarray(A_ele_gpu).flatten()
A = coo_matrix((V, (I, J)), shape=(len(node), len(node))).tocsr()

end_assem = time.time()
elapsed_assem = end_assem - start_assem

# ----------------------------------------------------------------
# 4. GPU CG Solver (경계 조건 적용 및 풀이)
# ----------------------------------------------------------------
# Dirichlet Boundary Condition (0 또는 1인 지점)
bd_mask = cp.any(cp.isclose(node_cp, 0) | cp.isclose(node_cp, 1), axis=1)
in_idx = cp.where(~bd_mask)[0]

A_reduced = A[in_idx, :][:, in_idx]
f_reduced = b[in_idx]

it_count = 0
def callback_func(xk):
    global it_count
    it_count += 1

start_solving = time.time()
u_reduced, info = cg(A_reduced, f_reduced, rtol=1e-6, callback=callback_func)

u_h = cp.zeros(len(node))
u_h[in_idx] = u_reduced
end_solving = time.time()
elapsed_solving = end_solving - start_solving

# ----------------------------------------------------------------
# 5. L2 Error 계산
# ----------------------------------------------------------------
u_exact = cp.sin(cp.pi * node_cp[:,0]) * cp.sin(cp.pi * node_cp[:,1]) * cp.sin(cp.pi * node_cp[:,2])
error = u_exact - u_h

# 질량 행렬(M) 조립을 통한 L2 norm 계산
phi_func_w = phi_func * cp.asarray(w)
det_val = cp.asarray(det_gpu)
e_m = ((phi_func @ phi_func_w.T)[:,:,cp.newaxis] @ det_val[cp.newaxis,:]).transpose(2,0,1)
M_global = csr_matrix((e_m.flatten(), (I, J)), shape=(len(node), len(node)))

L2error = cp.sqrt(error.T @ M_global @ error)

# ----------------------------------------------------------------
# 6. 결과 출력
# ----------------------------------------------------------------
print('-'*17, '최종 결과', '-'*17)
print(f"Preprocessing 시간 : {elapsed_pre:.4f} 초")
print(f"Assembly 시간      : {elapsed_assem:.4f} 초")
print(f"CG Solve 시간      : {elapsed_solving:.4f} 초")
print(f"Total 소요 시간    : {time.time() - start_total:.4f} 초")
print(f"CG ITERATION       : {it_count}")
print(f"L2 error           : {L2error.item():.4e}")
print('-'*43)
